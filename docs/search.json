[
  {
    "objectID": "posts/post5/index.html",
    "href": "posts/post5/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "The objective is to learn what “normal” data looks like, and then use that to detect abnormal instances. These instances are called anomalies, or outliers, while the normal instances are called inliers. Anomaly detection is useful in a wide variety of applications, such as fraud detection, detecting defective products in manufacturing, identifying new trends in time series, or removing outliers from a dataset before training another model, which can significantly improve the performance of the resulting model."
  },
  {
    "objectID": "posts/post3/index.html",
    "href": "posts/post3/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/post1/index.html",
    "href": "posts/post1/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Britney Aiken",
    "section": "",
    "text": "Virginia Tech M.Eng Computer Science Student"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS 5805 Blog Project",
    "section": "",
    "text": "Probability Theory and Random Variables\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nBritney Aiken\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nBritney Aiken\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nBritney Aiken\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/Outlier Detection\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nBritney Aiken\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nBritney Aiken\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post2/index.html",
    "href": "posts/post2/index.html",
    "title": "Clustering",
    "section": "",
    "text": "### Whether you’re a seasoned data scientist or a curious enthusiast, understanding the basics of clustering algorithms can open doors to a deeper comprehension of how data organizes itself. In this post, we will cover cluersting algorithms and their applications.\n\nWhat is clustering?\nData clustering is a method used in machine learning to organize large datasets into distinct groups or clusters. The goal is to group data points based on similarities and reveal patterns that might not be obvious at first.\n\n\nHow is clustering used?\nClustering is used in a wide variety of applications, including:\nCustomer Segmentation: Identifying groups of customers with similar preferences helps businesses tailor marketing strategies more effectively.\nImage and Signal Processing: Grouping similar pixels in an image or patterns in signals aids in image recognition and signal analysis.\nAnomaly Detection: Detecting unusual patterns or outliers in a dataset, which can be indicative of errors or potential threats.\nBiology and Genetics: Classifying genes based on similar expressions or grouping biological specimens for research purposes.\nDocument Clustering and Search Engines: Organizing large document collections by topic or theme for efficient information retrieval.\n\n\nK-Means Algorithm\nK-Means clustering is an unsupervised machine learning algorithm used to organize a dataset into distinct clusters. It is a simple algorithm with the folowing steps:\n\nInitialize: Randomly select k data points/instances as initial centroids.\nAssign: Calculate the distance of each data point/instance to each centroids and assign each point (the instance label) to the cluster with the nearest centroid.\nUpdate: Recalculate the mean of the data points in each cluster and update the centroids.\nRepeat: Iterate through steps 2 and 3 until the centroids stabilize.\n\n\nAdvanatages and Limitations\nThe simplicity of K-means makes it efficient and capable of handling large datasets. It also scales well as the number of data points grows. However, there are some limitations. The algorithm is guaranteed to converge, but it might not converge to the right solution. The outcome is influenced by the centroids that were randomly chosen during the initialization step. Moreover, k-means does not behave very well when the clusters have varying sizes, different densities, or nonspherical shapes.\n\n\nImprovements\nOne imrovement to limit the influence of the inintial centroids is to run the algorithm multiple times with different initializations and keep the best one. The best solution is determined by the model’s inertia (the sum of the squared distances between the instances and their closest centroids). A lower interia means a better model.\nAnother improvement, k-means++, selects centroids that are far from one another to lessen the likelyhood that the algortithm converges to a suboptimal solution. This improvement drastically reduces the number of times the algorithm needs to be run to find the optimal solution.\nA third improvement by Charles Elkan accelerates the algorithm by avoiding many unnecessary distance calculations. Howver, depending on the dataset, this implementation may actually slow down the trainig instead of mkaing it faster.\nA fourth improvement by David Sculley uses mini-batches instead of the full dataset at each iteration. This method makes it possible to cluster datasets that do not fit in memory.\n\n\nHow to determine the number of clusters\nTo determine the optimal number of clusters (k), we use the silhouette score. An instance’s silhouette score is equal to (b – a) / max(a, b), where a is the mean distance to the other instances in the same cluster and b is the mean nearest-cluster distance. The silhouette score can ranges between –1 and +1. A score close to +1 means that the instance is well inside its own cluster and far from other clusters, while a score close to 0 means that it is close to a cluster boundary. A score close to –1 means that the instance may have been assigned to the wrong cluster.\n\n\n\nDBSCAN\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a another clustering algorithm. It identifies clusters in a dataset based on the density of the data points. Unlike K-Means, DBSCAN doesn’t require specifying the number of clusters beforehand and is better suited for irregularly shaped clusters. The algorithm classifies datapoints as core, border, or noise.\nCore Points: A data point that has at least the minimum number of data points within a defined radius (a neighborhood). All data points in the neighborhood belong to the same cluster.\nBorder Points: A data point that is within the neighborhood of a core point but does not have enough neighbors to be considered a core point.\nNoise Points: Data points that are neither core points or border points.\n\nAdvanatages and Limitations\nDBSCAN is capable of identifying any number of clusters of any shape, and it is not senistive to outliers. However, if the density varies significantly across the clusters, or if there’s no low-density region around the clusters, DBSCAN may struggle to identify the clusters. This algorithm also does not scale well to large datasets."
  },
  {
    "objectID": "posts/post4/index.html",
    "href": "posts/post4/index.html",
    "title": "Classification",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/post4/index.html#data-classifcation-is-the-foundation-of-many-machine-learing-models.-data-classifcation-allows-us-to-make-informed-decisions-based-on-patterns-in-the-data.-in-this-blog-post-we-will-cover-the-importance-of-data-classifcation-various-classification-methods-and-the-metrics-used-to-evaluate-their-performance.",
    "href": "posts/post4/index.html#data-classifcation-is-the-foundation-of-many-machine-learing-models.-data-classifcation-allows-us-to-make-informed-decisions-based-on-patterns-in-the-data.-in-this-blog-post-we-will-cover-the-importance-of-data-classifcation-various-classification-methods-and-the-metrics-used-to-evaluate-their-performance.",
    "title": "Classification",
    "section": "Data classifcation is the foundation of many machine learing models. Data classifcation allows us to make informed decisions based on patterns in the data. In this blog post, we will cover the importance of data classifcation, various classification methods, and the metrics used to evaluate their performance.",
    "text": "Data classifcation is the foundation of many machine learing models. Data classifcation allows us to make informed decisions based on patterns in the data. In this blog post, we will cover the importance of data classifcation, various classification methods, and the metrics used to evaluate their performance."
  },
  {
    "objectID": "posts/post4/index.html#what-is-data-classification-why-is-it-important-in-machine-learning",
    "href": "posts/post4/index.html#what-is-data-classification-why-is-it-important-in-machine-learning",
    "title": "Classification",
    "section": "What is Data Classification? Why is it Important in Machine Learning?",
    "text": "What is Data Classification? Why is it Important in Machine Learning?\nData classification is the process of organizing and labeling data into predefined categories or classes. The primary objective is to train a machine learning model to recognize patterns within the data and accurately assign new instances to the appropriate class. The importance of data classification in machine learning lies in its ability to enable intelligent decision-making. By categorizing data, models can generalize from past experiences to make predictions or classifications on new, unseen data. This capability forms the foundation for a wide array of applications that impact our daily lives. conda update -n base -c defaults conda\n\n\nCode\n# Setup Scikit-Learn\nimport sys\nfrom packaging import version\nimport sklearn\n\n\n# Setup data visualization\nimport matplotlib.pyplot as plt\n\ndef plot_digit(image_data):\n    image = image_data.reshape(28, 28)\n    plt.imshow(image, cmap=\"binary\")\n    plt.axis(\"off\")\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n# Import the dataset\nfrom sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', as_frame=False)\nX, y = mnist.data, mnist.target\n\n\n# Plot example data\nplt.figure(figsize=(9, 9))\nfor idx, image_data in enumerate(X[:100]):\n    plt.subplot(10, 10, idx + 1)\n    plot_digit(image_data)\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.show()\n\n\nC:\\Users\\britn\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn("
  },
  {
    "objectID": "posts/post4/index.html#binary-classifiers",
    "href": "posts/post4/index.html#binary-classifiers",
    "title": "Classification",
    "section": "Binary Classifiers",
    "text": "Binary Classifiers\nBinary classification involves categorizing instances into one of two classes. These classes are usually boolean values(true or false; yes or no, 0 or 1)."
  },
  {
    "objectID": "posts/post4/index.html#performance-measures",
    "href": "posts/post4/index.html#performance-measures",
    "title": "Classification",
    "section": "Performance Measures",
    "text": "Performance Measures\nEvaluating the performance of a classifier is crucial to understand how well it generalizes to new, unseen data. Common performance measures include accuracy, precision, recall, F1 score, and the ROC-AUC curve. These metrics provide insights into the classifier’s strengths and weaknesses,"
  },
  {
    "objectID": "posts/post4/index.html#multiclass-classification",
    "href": "posts/post4/index.html#multiclass-classification",
    "title": "Classification",
    "section": "Multiclass Classification",
    "text": "Multiclass Classification\nIn multiclass classification, instances are assigned to one of multiple classes. Unlike binary classification, where there are only two possible outcomes, multiclass scenarios involve distinguishing among multiple classes."
  }
]