[
  {
    "objectID": "posts/regression/index.html",
    "href": "posts/regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Regression is used in many fields, including finance, economics, biology, and engineering. The goal is to predict a continuous outcome on historical or observed data.\n\n\n\nWhat is regresssion?\nRegression is a type of supervised learning task where the goal is to predict a continuous variable based on the input features. In other words, regression models are designed to establish a relationship between the input features and the target variable.\nTarget Variable (Dependent Variable): a continuous numeric value that the model tries to estimate.\nInput Features (Independent Variables): the model learns how these features relate to the target variable during the training process.\n\n\nTypes of Regression\n\nLinear Regression\nA linear regression model makes a prediction by calculating a weighted sum of the input features, plus a constant called the bias term. The relationship is modeled as a straight line.\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\nrnstate = np.random.RandomState(3)\ndata = 10 * rnstate.rand(100)\ntarget = 2 * data - 3 + rnstate.randn(100)\nplt.scatter(data, target);\n\nlinearModel = LinearRegression(fit_intercept=True)\nlinearModel.fit(data[:, np.newaxis], target)\nxfit = np.linspace(0, 10, 1000)\nyfit = linearModel.predict(xfit[:, np.newaxis])\nplt.plot(xfit, yfit);\nplt.show()\n\n\n\n\n\n\n\nPolynomial Regression\nPolynomial regression allows for more complex relationships by adding polynomial terms into the regression equation.\n\n\nRidge Regression and Lasso Regression\nRidge and Lasso regression are variations of linear regression that include regularization terms to prevent overfitting. Ridge regression adds a penalty term to the squared magnitude of coefficients, while Lasso regression adds a penalty term to the absolute magnitude.\n\n\nElastic Net Regression\nElastic Net regression is a compromise between ridge regression and lasso regression. The regularization term is a weighted sum of both ridge and lasso’s regularization terms. You can control the mix ratio r. When r = 0, elastic net is equivalent to ridge regression, and when r = 1, it is equivalent to lasso regression.\n\n\nLogistic Regression\nLogistic regression is used to estimate the probability that an instance belongs to a particular class. If the estimated probability is greater than a given threshold, then the model predicts that the instance belongs to that class. This makes it a binary classifier. Similar to a linear regression model, a logistic regression model calculates the weighted sum of the input features, but instead of returning the result, it rturns the logistic of the result.\n\n\nSoftmax Regression\nSoftmax regression is a generalization of logistic regression to support multiple classes without having to train and combine multiple binary classifiers. The algorithm computes a score for each class, estimates the probability that an instance belongs to the class, and predicts the class with the highest probability.\n\n\n\nPerformance Metrics\nMean Squared Error (MSE): Measures the average squared difference between predicted and true values. This is used to assessing the accuracy of a model.\nMean Absolute Error (MAE): Measures the average absolute difference between predicted and true values. It is less sensitive to outliers than MSE.\nR-squared (R²): Represents the proportion of the variance in the target variable that is predictable from the input features. Ranges from 0 to 1, where 1 indicates a perfect fit.\n\n\nGradient Descent\nGradient Descent is an optimization algorithm used to minimize a cost function by adjusting the weights of a model. The algorithm follows these steps:\n\nStart with random values for the model parameters.\nCalculate the gradient of the cost function. The gradient indicates the direction of the steepest increase\nAdjust the model parameters in the opposite direction of the gradient to decrease the cost function.\nRepeat steps 2 and 3 until the algorithm converges to a minimum\n\nThe algorith improves gradually, taking one step at a time. The size of the steps is determined by the learning rate. If the learning rate is too small, the algorithm will take a long time to converge. If the learning rate is too high, the algorithm will miss key patterns in the data.\n\n\nEarly Stopping\nPolynomial regression algorithms use a technique called Early Stopping to prevent overfitting. As an algorithm learns, its prediction and validation errors goes down. But after a while, the validation error will start to go back up. This indicates that the model is starting to overfit the data. With early stopping, you stop training as soon as the validation error reaches the minimum."
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Whether you’re a seasoned data scientist or a curious enthusiast, understanding the basics of clustering algorithms can open doors to a deeper comprehension of how data organizes itself.\n\n\n\nWhat is clustering?\nData clustering is a method used in machine learning to organize large datasets into distinct groups or clusters. The goal is to group data points based on similarities and reveal patterns that might not be obvious at first.\n\n\nHow is clustering used?\nClustering is used in a wide variety of applications, including:\nCustomer Segmentation: Identifying groups of customers with similar preferences helps businesses tailor marketing strategies more effectively.\nImage and Signal Processing: Grouping similar pixels in an image or patterns in signals aids in image recognition and signal analysis.\nAnomaly Detection: Detecting unusual patterns or outliers in a dataset, which can be indicative of errors or potential threats.\nBiology and Genetics: Classifying genes based on similar expressions or grouping biological specimens for research purposes.\nDocument Clustering and Search Engines: Organizing large document collections by topic or theme for efficient information retrieval.\n\n\nK-Means Algorithm\nK-Means clustering is an unsupervised machine learning algorithm used to organize a dataset into distinct clusters. It is a simple algorithm with the folowing steps:\n\nInitialize: Randomly select k data points/instances as initial centroids.\nAssign: Calculate the distance of each data point/instance to each centroids and assign each point (the instance label) to the cluster with the nearest centroid.\nUpdate: Recalculate the mean of the data points in each cluster and update the centroids.\nRepeat: Iterate through steps 2 and 3 until the centroids stabilize.\n\n\n\nCode\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\n\ndataset = fetch_openml('S3', as_frame=False)\ndata = dataset.data[:800]\n\nk = 15\nkmeans = KMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(data)\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\nplt.figure(figsize=(8, 4))\nplot_clusters(data)\nplt.gca().set_axisbelow(True)\nplt.grid()\nplt.show()\n\n\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\nAdvanatages and Limitations\nThe simplicity of K-means makes it efficient and capable of handling large datasets. It also scales well as the number of data points grows. However, there are some limitations. The algorithm is guaranteed to converge, but it might not converge to the right solution. The outcome is influenced by the centroids that were randomly chosen during the initialization step. Moreover, k-means does not behave very well when the clusters have varying sizes, different densities, or nonspherical shapes.\n\n\nCode\ndef plot_data(X):\n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\n\ndef plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n                             show_xlabels=True, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                cmap=\"tab20\")\n    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                linewidths=1, colors='k')\n    plot_data(X)\n    if show_centroids:\n        plot_centroids(clusterer.cluster_centers_)\n\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\n\ndef plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):\n    clusterer1.fit(X)\n    clusterer2.fit(X)\n\n    plt.figure(figsize=(10, 3.2))\n\n    plt.subplot(121)\n    plot_decision_boundaries(clusterer1, X)\n    if title1:\n        plt.title(title1)\n\n    plt.subplot(122)\n    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n    if title2:\n        plt.title(title2)\n\nClusters1 = KMeans(n_clusters=3, init=\"random\", n_init=1, random_state=2)\nClusters1.fit_predict(data)\n\nClusters2 = KMeans(n_clusters=3, init=\"random\", n_init=1, random_state=30)\nClusters2.fit_predict(data)\n\nplot_clusterer_comparison(Clusters1, Clusters2, data, \"Solution with a random init\", \"Solution with a different random init\")\n\nplt.show()\n\n\n\n\n\n\n\nImprovements\nOne imrovement to limit the influence of the inintial centroids is to run the algorithm multiple times with different initializations and keep the best one. The best solution is determined by the model’s inertia (the sum of the squared distances between the instances and their closest centroids). A lower interia means a better model.\nAnother improvement, k-means++, selects centroids that are far from one another to lessen the likelyhood that the algortithm converges to a suboptimal solution. This improvement drastically reduces the number of times the algorithm needs to be run to find the optimal solution.\nA third improvement by Charles Elkan accelerates the algorithm by avoiding many unnecessary distance calculations. Howver, depending on the dataset, this implementation may actually slow down the trainig instead of mkaing it faster.\nA fourth improvement by David Sculley uses mini-batches instead of the full dataset at each iteration. This method makes it possible to cluster datasets that do not fit in memory.\n\n\nHow to determine the number of clusters\nTo determine the optimal number of clusters (k), we use the silhouette score. An instance’s silhouette score is equal to (b – a) / max(a, b), where a is the mean distance to the other instances in the same cluster and b is the mean nearest-cluster distance. The silhouette score can ranges between –1 and +1. A score close to +1 means that the instance is well inside its own cluster and far from other clusters, while a score close to 0 means that it is close to a cluster boundary. A score close to –1 means that the instance may have been assigned to the wrong cluster.\n\n\nCode\nkClusters1 = KMeans(n_clusters=3)\nkClusters1.fit_predict(data)\n\nkClusters2 = KMeans(n_clusters=4)\nkClusters2.fit_predict(data)\n\nplot_clusterer_comparison(kClusters1, kClusters2, data, \"$k=3$\", \"$k=4$\")\nplt.show()\n\n\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\n\nDBSCAN\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a another clustering algorithm. It identifies clusters in a dataset based on the density of the data points. Unlike K-Means, DBSCAN doesn’t require specifying the number of clusters beforehand and is better suited for irregularly shaped clusters. The algorithm classifies datapoints as core, border, or noise.\nCore Points: A data point that has at least the minimum number of data points within a defined radius (a neighborhood). All data points in the neighborhood belong to the same cluster.\nBorder Points: A data point that is within the neighborhood of a core point but does not have enough neighbors to be considered a core point.\nNoise Points: Data points that are neither core points or border points.\n\nAdvanatages and Limitations\nDBSCAN is capable of identifying any number of clusters of any shape, and it is not senistive to outliers. However, if the density varies significantly across the clusters, or if there’s no low-density region around the clusters, DBSCAN may struggle to identify the clusters. This algorithm also does not scale well to large datasets."
  },
  {
    "objectID": "posts/anomaly detection/index.html",
    "href": "posts/anomaly detection/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "Anomaly detection is useful in a wide variety of applications, such as fraud detection, detecting defective products, and cybersecurity.\n\n\n\nWhat is Anomaly Detection?\nThe goal is to learn what “normal” data looks like, and then use that to detect abnormal instances. These instances are called anomalies or outliers.\n\n\nWhy is it important?\nAnomaly detection is a critical tool for maintaining the integrity and security of a system. It enables early identification of unusual patterns that may causes issues.\n\n\nAlgorithms for Anamoly Detction\nGaussian Mixture Anomaly detection assumes that normal instances occur more often than outliers. The algorithm starts by learning the patterns of normal behavior and analzes the datapoint that do not fit the pattern. When using a Gaussian mixture model for anomaly detection, any instance located in a low-density region can be considered an anomaly.\n\n\nCode\nfrom sklearn.mixture import GaussianMixture\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\n\ndataset = fetch_openml('S3', as_frame=False)\ndata = dataset.data[:800]\n\ngm = GaussianMixture().fit(data)\ndensities = gm.score_samples(data)\ndensity_threshold = np.percentile(densities, 2)\nanomalies = data[densities &lt; density_threshold]\n\nplt.scatter(data[:, 0], data[:, 1])\nplt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='*')\nplt.show()\n\n\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n\n\n\n\n\nReferences: [@Fränti]\nFast-MCD (Minimum Covariance Determinant) This algorithm is useful for outlier detection, especially when trying to clean up a dataset. When the algorithm estimates the parameters of the Gaussian distribution, it ignores the instances that are most likely outlier, making it easier to identify them.\nIsolation Forest This algorithm works well in high-dimensional datasets. It builds a random forest where each decision tree grows randomly. The datapoints gradually spread apart causing the anomalies to become isolated much fater than normal datapoints.\nLocal outlier factor (LOF) This algorithm compares the density of instances around a given instance to the density around its neighbors. An anomaly is often more isolated than its k-nearest neighbors.\nOne-class SVM This algorithm is works well for novelty detection. Novelty detection differs from anomaly detection by assuming the algorithm was trained on a “clean” dataset, with no outliers. One-class SVM works by finding a small region that encompasses all the instances. If a new instance does not fall within this region, it is an anomaly\n\n\nReferences\nFränti , P, and O Virmajoki. “S3.” OpenML, openml.org/search?type=data&status=active&sort=qualities.NumberOfNumericFeatures&id=42112. Accessed 15 Dec. 2023."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Britney Aiken",
    "section": "",
    "text": "Virginia Tech M.Eng Computer Science Student"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS 5805 Blog Project",
    "section": "",
    "text": "Anomaly/Outlier Detection\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nBritney Aiken\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nBritney Aiken\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nBritney Aiken\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nBritney Aiken\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nBritney Aiken\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Data classifcation is the foundation of many machine learing models. It allows us to make informed decisions based on patterns in the data.\n\n\n\nWhat is Data Classification? Why is it Important in Machine Learning?\nData classification is the process of organizing and labeling data into predefined categories or classes. The goal is to train a machine learning model to recognize patterns within the data and accurately assign new instances to the appropriate class. The importance of data classification in machine learning lies in its ability to enable intelligent decision-making. By categorizing data, models can generalize from past experiences to make predictions or classifications on new, unseen data. This capability forms the foundation for a wide array of applications that impact our daily lives.\n\n\nCode\nimport sys\nfrom packaging import version\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nimport matplotlib.pyplot as plt\n\n# Import the dataset\nfashion_data = fetch_openml('fashion-mnist', as_frame=False)\ndata = fashion_data.data \ntarget = fashion_data.target\n\n\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n\n\nReferences: [@Xiao]\n\n\nCode\n# Setup data visualization\ndef plot_digit(image_data):\n    image = image_data.reshape(28, 28)\n    plt.imshow(image, cmap=\"binary\")\n    plt.axis(\"off\")\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n\n# Plot example data\nplt.figure(figsize=(9, 9))\nfor idx, image_data in enumerate(data[:10]):\n    plt.subplot(10, 10, idx + 1)\n    plot_digit(image_data)\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.show()\n\n\n\n\n\nThe figure above shows a sample of the fashion mnist dataset. The calculations and visuals below will reference this dataset. References: [@Geron]\n\n\nBinary Classifiers\nBinary classification involves categorizing instances into one of two classes. These classes are usually boolean values(true or false; yes or no, 0 or 1). Evaluating the performance of a classifier is crucial to understand how well it generalizes to new data. Common performance measures include accuracy, precision, recall, F1 score, and the ROC-AUC curve. These metrics provide insights into the classifier’s strengths and weaknesses,\n\nConfusion Matrix\nA confusion matrix provides a detailed breakdown of the model’s predictions and the actual outcomes for each class. The four components of a confusion matrix are true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These components are used to calculate performance metrics for a binary classifier.\nTrue Positives (TP): The number of instances that are actually positive and are correctly predicted as positive\nTrue Negatives (TN): The number of instances that are actually negative and are correctly predicted as negative\nFalse Positives (FP): The number of instances that are actually negative but are incorrectly predicted as positive (Type 1 error)\nFalse Negatives (FN): The number of instances that are actually positive but are incorrectly predicted as negative (Type 2 error)\nUsing these components, we can calculate several performance metrics.\nAccuracy: The proportion of correctly classified instances out of the total instances. (TP+TN)/(TP+TN+FP+FN)\nPrecision The ratio of true positives to the total predicted positives. TP/(TP+FP)\n\n\nCode\nfrom sklearn.metrics import precision_score\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import cross_val_predict\n\n# create the target vector\nxTrain, xTest, yTrain, yTest = data[:1000], data[1000:], target[:1000], target[1000:]\nyTrainGood = (yTrain == '7')  \nyTestGood = (yTest == '7')\n\n# train the classifier\nclassifier = SGDClassifier()\nclassifier.fit(xTrain, yTrainGood)\nyTrainPrediction = cross_val_predict(classifier, xTrain, yTrainGood, cv=3)\n\n# get precision score\nprint(\"The presicion score after training the binary classifier: \", precision_score(yTrainGood, yTrainPrediction))\n\n\nThe presicion score after training the binary classifier:  0.860655737704918\n\n\nRecall: The ratio of true positives to the total actual positives. TP/(TP+FN) This metric is also known as Sensitivity or True Positive Rate\n\n\nCode\nfrom sklearn.metrics import recall_score\n\n# get recall score\nprint(\"The recall score of the classifier: \", recall_score(yTrainGood, yTrainPrediction))\n\n\nThe recall score of the classifier:  0.9130434782608695\n\n\nSpecificity: The ratio of true negatives to the total actual negatives. TN/(TN+FP) This metric is also known as the True Negative Rate\nF1 Score: The harmonic mean of precision and recall. Increasing precision reduces recall, and vice versa. This is called the precision/recall trade-off. 2×(Precision×Recall)/(Precision+Recall)\n\n\nCode\nfrom sklearn.metrics import f1_score\n\n# get F1 score\nprint(\"The F1 score of the classifier: \", f1_score(yTrainGood, yTrainPrediction))\n\n\nThe F1 score of the classifier:  0.8860759493670886\n\n\n\n\nThe Precsion/Recall (PR) Curve\nThe precision-recall curve is created by plotting precision against recall at different threshold values. Each point on the curve corresponds to a specific decision threshold used by the classifier to make predictions.\n\n\nCode\nfrom sklearn.metrics import precision_recall_curve\n\nthreshold = 1000\nyScores = cross_val_predict(classifier, xTrain, yTrainGood, cv=3, method=\"decision_function\")\n\n# plot and format the PR curve\nprecisions, recalls, thresholds = precision_recall_curve(yTrainGood, yScores)\nidx = (thresholds &gt; threshold).argmax() \n\nplt.figure(figsize=(6, 5)) \nplt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n\nplt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\nplt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\n\nplt.plot([recalls[idx]], [precisions[idx]], \"ko\",\n         label=\"Point at threshold 1000\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.axis([0, 1, 0, 1])\nplt.grid()\nplt.legend(loc=\"lower left\")\n\n\n&lt;matplotlib.legend.Legend at 0x2be25fb1fa0&gt;\n\n\n\n\n\n\n\nThe ROC Curve\nThe Receiver Operating Characteristic (ROC) curve is another tool used with binary classifiers. It is similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate (FPR).\n\n\nCode\nfrom sklearn.metrics import roc_curve\n\n# calculate roc\nprecisionIdx = (precisions &gt;= 0.90).argmax()\nthresholdPrecision = thresholds[precisionIdx]\nfalsePositiveRate, truePositiveRate, thresholds = roc_curve(yTrainGood, yScores)\nidxThreshold = (thresholds &lt;= thresholdPrecision).argmax()\ntruePositiveRate90, falsePositiveRate90 = truePositiveRate[idxThreshold], falsePositiveRate[idxThreshold]\n\n# plot the roc curve\nplt.figure(figsize=(6, 5))\nplt.plot(falsePositiveRate, truePositiveRate, linewidth=2, label=\"ROC curve\")\nplt.plot([falsePositiveRate90], [truePositiveRate90], \"ko\", label=\"Threshold for 90% precision\")\n\n# format the figure\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid()\nplt.axis([0, 1, 0, 1])\nplt.legend(loc=\"lower right\", fontsize=13)\nplt.show()\n\n\n\n\n\nThe higher the recall (TPR), the more false positives (FPR) the classifier produces. One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5.\n\n\n\nMulticlass Classification\nIn multiclass classification, instances are assigned to one of multiple classes. The model learns decision boundaries to separate instances belonging to different classes. The decision-making process involves assigning each instance to the class with the highest probability. Two strategies for adapting binary classifiers to multiclass problems are One-vs-All (OvA) and One-vs-One (OvO).\nOne-vs-All: Train a separate binary classifier for each class, treating it as the positive class and the rest as the negative class. The class with the highest score is then predicted.\nOne-vs-One: Train a binary classifier for every pair of classes. In the prediction phase, each classifier votes for a class, and the class with the most votes is the final prediction.\n\n\nOther Types of Classification\nMultilabel classification allows an instance to be associated with multiple labels or classes at the same time. This scenario is common in real-world problems where data points have many attributes.\nMultioutput Classification is a generalization of multilabel classification where each label can be multiclass\n\n\nReferences\nXiao, Han, et al. “Fashion-MNIST.” OpenML, openml.org/search?type=data&status=active&sort=qualities.NumberOfNumericFeatures&id=40996. Accessed 15 Dec. 2023.\nGéron, Aurélien. Hands-on Machine Learning with Scikit-Learn, Keras, and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media, 2022."
  },
  {
    "objectID": "posts/probability theory/index.html",
    "href": "posts/probability theory/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Probability theory and random variables provide the mathematical framework for understanding uncertainty, variability, and making informed predictions.\n\n\n\nWhat is Probability Theory?\nProbability theory is a branch of mathematics that deals with the quantification of uncertainty. It is used to model the uncertainty inherent when making predictions about data.\n\n\nWhat are Random Variables?\nRandom variables represent the set of possible outcomes of a random event. There are two types of random variables:\nDiscrete Random Variables: Discrete random variables represent a set number of distinct values. Examples include tossing a coin (heads or tails), or rolling dice (1,2,3,4,5,6).\nContinuous Random Variables: Continuous random represent an infinite number of possible values. Examples include the height of individuals, the temperature, or time.\nEach random variable is associated with a probability distribution, which describes the likelihood of the random variable taking on different values.\n\n\nProbability Distributions\nA probability distribution describes how the probabilities are spread across different values of a random variable.\nUniform Distribution: Assigns equal probability to all possible outcomes. This is used when we do not know the likelihood of different values.\nNormal Distribution: The probability density of a normal distribution (also known as the Gaussian distribution) looks like a bell-shaped curve. This curve reaches its peak at the mean and gradually decreases as values move away from the mean. Many algorithms assume that the data follows a normal distribution.\n\n\nApplications in Machine Learning\nBayesian Inference: Probability theory supports Bayesian methods, where probabilities are assigned to hypotheses, and the model is updated based on observed evidence.\nUncertainty Quantification: Probability theory helps quantify the uncertainty of a machine learing prediction. This can be used to analyze a model’s performance.\nStatistical Inference: Probability distributions and random variables are essential in hypothesis testing, parameter estimation, and drawing conclusions about populations from sample data.\nMonte Carlo Methods: Monte Carlo techniques rely on random sampling to estimate solutions to complex problems."
  }
]