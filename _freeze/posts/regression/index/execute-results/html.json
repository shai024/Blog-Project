{
  "hash": "7e40f160cad45a0144042334e10189d3",
  "result": {
    "markdown": "---\ntitle: \"Linear and Nonlinear Regression\"\nauthor: \"Britney Aiken\"\ndate: \"2023-12-04\"\nimage: \"regression.jpg\"\ncode-fold: true\ncode-tools: true\ncode-block-bg: true\ncode-block-border-left: \"#31BAE9\"\n---\n\n![Regression is used in many fields, including finance, economics, biology, and engineering. The goal is to predict a continuous outcome on historical or observed data.](regression.jpg)\n\n\n### What is regresssion? \nRegression is a type of supervised learning task where the goal is to predict a continuous variable based on the input features. In other words, regression models are designed to establish a relationship between the input features and the target variable.\n\n**Target Variable (Dependent Variable):** a continuous numeric value that the model tries to estimate.\n\n**Input Features (Independent Variables):** the model learns how these features relate to the target variable during the training process.\n\n### Types of Regression\n\n#### Linear Regression\nA linear regression model makes a prediction by calculating a weighted sum of the input features, plus a constant called the bias term. The relationship is modeled as a straight line.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\nrnstate = np.random.RandomState(3)\ndata = 10 * rnstate.rand(100)\ntarget = 2 * data - 3 + rnstate.randn(100)\nplt.scatter(data, target);\n\nlinearModel = LinearRegression(fit_intercept=True)\nlinearModel.fit(data[:, np.newaxis], target)\nxfit = np.linspace(0, 10, 1000)\nyfit = linearModel.predict(xfit[:, np.newaxis])\nplt.plot(xfit, yfit);\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\n#### Polynomial Regression\nPolynomial regression allows for more complex relationships by adding polynomial terms into the regression equation. \n\n\n#### Ridge Regression and Lasso Regression\nRidge and Lasso regression are variations of linear regression that include regularization terms to prevent overfitting. Ridge regression adds a penalty term to the squared magnitude of coefficients, while Lasso regression adds a penalty term to the absolute magnitude.\n\n#### Elastic Net Regression\nElastic Net regression is a compromise between ridge regression and lasso regression. The regularization term is a weighted sum of both ridge and lasso’s regularization terms. You can control the mix ratio `r`. When `r = 0`, elastic net is equivalent to ridge regression, and when `r = 1`, it is equivalent to lasso regression.\n\n#### Logistic Regression\nLogistic regression is used to estimate the probability that an instance belongs to a particular class. If the estimated probability is greater than a given threshold, then the model predicts that the instance belongs to that class. This makes it a binary classifier. Similar to a linear regression model, a logistic regression model calculates the weighted sum of the input features, but instead of returning the result, it rturns the logistic of the result.\n\n#### Softmax Regression\nSoftmax regression is a generalization of logistic regression to support multiple classes without having to train and combine multiple binary classifiers. The algorithm computes a score for each class, estimates the probability that an instance belongs to the class, and predicts the class with the highest probability. \n\n### Performance Metrics\n**Mean Squared Error (MSE):** Measures the average squared difference between predicted and true values. This is used to assessing the accuracy of a model.\n\n**Mean Absolute Error (MAE):** Measures the average absolute difference between predicted and true values. It is less sensitive to outliers than MSE.\n\n**R-squared (R²):** Represents the proportion of the variance in the target variable that is predictable from the input features. Ranges from 0 to 1, where 1 indicates a perfect fit.\n\n### Gradient Descent\nGradient Descent is an optimization algorithm used to minimize a cost function by adjusting the weights of a model. The algorithm follows these steps:\n\n1. Start with random values for the model parameters. \n2. Calculate the gradient of the cost function. The gradient indicates the direction of the steepest increase\n3. Adjust the model parameters in the opposite direction of the gradient to decrease the cost function.\n4. Repeat steps 2 and 3 until the algorithm converges to a minimum\n\nThe algorith improves gradually, taking one step at a time. The size of the steps is determined by the learning rate. If the learning rate is too small, the algorithm will take a long time to converge. If the learning rate is too high, the algorithm will miss key patterns in the data. \n\n### Early Stopping\nPolynomial regression algorithms use a technique called Early Stopping to prevent overfitting. As an algorithm learns, its prediction  and validation errors goes down. But after a while, the validation error will start to go back up. This indicates that the model is starting to overfit the data. With early stopping, you stop training as soon as the validation error reaches the minimum.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}