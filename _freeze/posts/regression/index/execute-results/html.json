{
  "hash": "250c5b0e586496e3321d71cd38327c79",
  "result": {
    "markdown": "---\ntitle: \"Linear and Nonlinear Regression\"\nauthor: \"Britney Aiken\"\ndate: \"2023-12-04\"\nimage: \"regression.jpg\"\ncode-fold: true\ncode-tools: true\ncode-block-bg: true\ncode-block-border-left: \"#31BAE9\"\n---\n\n![Regression is used in many fields, including finance, economics, biology, and engineering. The goal is to predict a continuous outcome on historical or observed data.](regression.jpg)\n\n\n### What is regresssion? \nRegression is a type of supervised learning task where the goal is to predict a continuous variable based on the input features. In other words, regression models are designed to establish a relationship between the input features and the target variable.\n\n**Target Variable (Dependent Variable):** a continuous numeric value that the model tries to estimate.\n\n**Input Features (Independent Variables):** the model learns how these features relate to the target variable during the training process.\n\n### Types of Regression\n\n#### Linear Regression\nA linear regression model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term (also called the intercept term).  The relationship is modeled as a straight line.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.preprocessing import add_dummy_feature\nimport matplotlib.pyplot as plt\n\n# create a graph of linear data\nnp.random.seed(42) \nm = 100 \nX = 2 * np.random.rand(m, 1)  \ny = 4 + 3 * X + np.random.randn(m, 1) \n\n# add features\nX_b = add_dummy_feature(X) \ntheta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n\n# make predictions\nX_new = np.array([[0], [2]])\nX_new_b = add_dummy_feature(X_new) \ny_predict = X_new_b @ theta_best\n\n# plot the data\nplt.figure(figsize=(6, 4)) \nplt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\nplt.plot(X, y, \"b.\")\n\n# format the figure\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.axis([0, 2, 0, 15])\nplt.grid()\nplt.legend(loc=\"upper left\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=517 height=356}\n:::\n:::\n\n\n#### Polynomial Regression\nAllows for more complex relationships by introducing polynomial terms into the regression equation. This is particularly useful when the relationship between variables is nonlinear.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# create data points\nnp.random.seed(42)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\n\n# add features\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)\n\n# fit the model\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly, y)\n\n# make predictions\nX_new = np.linspace(-3, 3, 100).reshape(100, 1)\nX_new_poly = poly_features.transform(X_new)\ny_new = lin_reg.predict(X_new_poly)\n\n# plot and format the graph\nplt.figure(figsize=(6, 4))\nplt.plot(X, y, \"b.\")\nplt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.legend(loc=\"upper left\")\nplt.axis([-3, 3, 0, 10])\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=506 height=361}\n:::\n:::\n\n\n#### Ridge Regression and Lasso Regression\nRidge and Lasso regression are variations of linear regression that include regularization terms to prevent overfitting. Ridge regression adds a penalty term to the squared magnitude of coefficients, while Lasso regression adds a penalty term to the absolute magnitude.\n\n#### Elastic Net Regression\nElastic net regression is a middle ground between ridge regression and lasso regression. The regularization term is a weighted sum of both ridge and lasso’s regularization terms, and you can control the mix ratio `r`. When `r = 0`, elastic net is equivalent to ridge regression, and when `r = 1`, it is equivalent to lasso regression\n\n#### Logistic Regression\nLogistic regression is used to estimate the probability that an instance belongs to a particular class. If the estimated probability is greater than a given threshold, then the model predicts that the instance belongs to that class. This makes it a binary classifier. Similar to a linear regression model, a logistic regression model calculates the weighted sum of the input features, but instead of returning the result, it rturns the logistic of the result.\n\n#### Softmax Regression\nSoftmax regression is a generalization of logistic regression to support multiple classes without having to train and combine multiple binary classifiers. The algorithm computes a score for each class, estimates the probability that an instance belongs to the class, and predicts the class with the highest probability. \n\n### Performance Metrics\n**Mean Squared Error (MSE):** Measures the average squared difference between predicted and true values. This is used to assessing the accuracy of a model.\n\n**Mean Absolute Error (MAE):** Measures the average absolute difference between predicted and true values. It is less sensitive to outliers than MSE.\n\n**R-squared (R²):** Represents the proportion of the variance in the target variable that is predictable from the input features. Ranges from 0 to 1, where 1 indicates a perfect fit.\n\n### Gradient Descent\nGradient Descent is an optimization algorithm used to minimize a cost function by adjusting the weights of a model. The algorithm follows these steps:\n\n1. Start with random values for the model parameters. \n2. Calculate the gradient of the cost function. The gradient indicates the direction of the steepest increase\n3. Adjust the model parameters in the opposite direction of the gradient to decrease the cost function.\n4. Repeat steps 2 and 3 until the algorithm converges to a minimum\n\nThe algorith improves gradually, taking one step at a time. The size of the steps is determined by the learning rate. If the learning rate is too small, the algorithm will take a long time to converge. If the learning rate is too high, the algorithm will miss key patterns in the data. \n\n### Early Stopping\nPolynomial regression algorithms use a technique called Early Stopping to prevent overfitting. As an algorithm learns, its prediction error (RMSE) goes down. After a while, though, the validation error starts to go back up. This indicates that the model has started to overfit the data. With early stopping, you stop training as soon as the validation error reaches the minimum.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}