{
  "hash": "bd61120ca085e2ecc1abbc6d103ec540",
  "result": {
    "markdown": "---\ntitle: \"Clustering\"\nauthor: \"Britney Aiken\"\ndate: \"2023-12-01\"\nimage: \"clustering.png\"\ncode-fold: true\ncode-tools: true\ncode-block-bg: true\ncode-block-border-left: \"#31BAE9\"\n---\n\n![Whether you're a seasoned data scientist or a curious enthusiast, understanding the basics of clustering algorithms can open doors to a deeper comprehension of how data organizes itself.](clustering.png)\n\n### What is clustering? \nData clustering is a method used in machine learning to organize large datasets into distinct groups or clusters. The goal is to group data points based on similarities and reveal patterns that might not be obvious at first. \n\n### How is clustering used?\nClustering is used in a wide variety of applications, including:\n\n**Customer Segmentation:**\nIdentifying groups of customers with similar preferences helps businesses tailor marketing strategies more effectively.\n\n**Image and Signal Processing:**\nGrouping similar pixels in an image or patterns in signals aids in image recognition and signal analysis.\n\n**Anomaly Detection:**\nDetecting unusual patterns or outliers in a dataset, which can be indicative of errors or potential threats.\n\n**Biology and Genetics:**\nClassifying genes based on similar expressions or grouping biological specimens for research purposes.\n\n**Document Clustering and Search Engines:**\nOrganizing large document collections by topic or theme for efficient information retrieval.\n\n\n\n### K-Means Algorithm\nK-Means clustering is an unsupervised machine learning algorithm used to organize a dataset into distinct clusters. \nIt is a simple algorithm with the folowing steps:\n\n1. **Initialize:** Randomly select `k` data points/instances as initial centroids.\n2. **Assign:** Calculate the distance of each data point/instance to each centroids and assign each point (the instance label) to the cluster with the nearest centroid.\n3. **Update:** Recalculate the mean of the data points in each cluster and update the centroids.\n4. **Repeat:** Iterate through steps 2 and 3 until the centroids stabilize.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\n\ndataset = fetch_openml('S3', as_frame=False)\ndata = dataset.data[:800]\n\nk = 15\nkmeans = KMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(data)\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\nplt.figure(figsize=(8, 4))\nplot_clusters(data)\nplt.gca().set_axisbelow(True)\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){}\n:::\n:::\n\n\n#### Advanatages and Limitations\nThe simplicity of K-means makes it efficient and capable of handling large datasets. It also scales well as the number of data points grows. \nHowever, there are some limitations. The algorithm is guaranteed to converge, but it might not converge to the right solution. The outcome is influenced by the centroids that were randomly chosen during the initialization step. Moreover, k-means does not behave very well when the clusters have varying sizes, different densities, or nonspherical shapes. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef plot_data(X):\n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights > weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\n\ndef plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n                             show_xlabels=True, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                cmap=\"tab20\")\n    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                linewidths=1, colors='k')\n    plot_data(X)\n    if show_centroids:\n        plot_centroids(clusterer.cluster_centers_)\n\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\n\ndef plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):\n    clusterer1.fit(X)\n    clusterer2.fit(X)\n\n    plt.figure(figsize=(10, 3.2))\n\n    plt.subplot(121)\n    plot_decision_boundaries(clusterer1, X)\n    if title1:\n        plt.title(title1)\n\n    plt.subplot(122)\n    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n    if title2:\n        plt.title(title2)\n\nClusters1 = KMeans(n_clusters=3, init=\"random\", n_init=1, random_state=2)\nClusters1.fit_predict(data)\n\nClusters2 = KMeans(n_clusters=3, init=\"random\", n_init=1, random_state=30)\nClusters2.fit_predict(data)\n\nplot_clusterer_comparison(Clusters1, Clusters2, data, \"Solution with a random init\", \"Solution with a different random init\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\n#### Improvements\nOne imrovement to limit the influence of the inintial centroids is to run the algorithm multiple times with different initializations and keep the best one. The best solution is determined by the model's `inertia` (the sum of the squared distances between the instances and their closest centroids). A lower interia means a better model. \n\nAnother improvement, k-means++, selects centroids that are far from one another to lessen the likelyhood that the algortithm converges to a suboptimal solution. This improvement drastically reduces the number of times the algorithm needs to be run to find the optimal solution. \n\nA third improvement by Charles Elkan accelerates the algorithm by avoiding many unnecessary distance calculations. Howver, depending on the dataset, this implementation may actually slow down the trainig instead of mkaing it faster. \n\nA fourth improvement by David Sculley uses mini-batches instead of the full dataset at each iteration. This method makes it possible to cluster  datasets that do not fit in memory. \n\n#### How to determine the number of clusters\nTo determine the optimal number of clusters (`k`), we use the silhouette score. An instance’s silhouette score is equal to `(b – a) / max(a, b)`, where `a` is the mean distance to the other instances in the same cluster and `b` is the mean nearest-cluster distance. The silhouette score can ranges between –1 and +1. A score close to +1 means that the instance is well inside its own cluster and far from other clusters, while a score close to 0 means that it is close to a cluster boundary. A score close to –1 means that the instance may have been assigned to the wrong cluster.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nkClusters1 = KMeans(n_clusters=3)\nkClusters1.fit_predict(data)\n\nkClusters2 = KMeans(n_clusters=4)\nkClusters2.fit_predict(data)\n\nplot_clusterer_comparison(kClusters1, kClusters2, data, \"$k=3$\", \"$k=4$\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){}\n:::\n:::\n\n\n### DBSCAN\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a another clustering algorithm. It identifies clusters in a dataset based on the density of the data points. Unlike K-Means, DBSCAN doesn't require specifying the number of clusters beforehand and is better suited for irregularly shaped clusters. The algorithm classifies datapoints as core, border, or noise. \n\n**Core Points:**\nA data point that has at least the minimum number of data points within a defined radius (a neighborhood). All data points in the neighborhood belong to the same cluster.\n\n**Border Points:**\nA data point that is within the neighborhood of a core point but does not have enough neighbors to be considered a core point.\n\n**Noise Points:**\nData points that are neither core points or border points.\n\n\n#### Advanatages and Limitations\nDBSCAN is capable of identifying any number of clusters of any shape, and it is not senistive to outliers. However, if the density varies significantly across the clusters, or if there’s no low-density region around the clusters, DBSCAN may struggle to identify the clusters. This algorithm also does not scale well to large datasets.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}