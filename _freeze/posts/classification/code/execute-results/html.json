{
  "hash": "d5f0d7699b25a9d60b80640189f9c993",
  "result": {
    "markdown": "::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport sys\nfrom packaging import version\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nimport matplotlib.pyplot as plt\n\n# Import the dataset\nfashion_data = fetch_openml('fashion-mnist', as_frame=False)\ndata = fashion_data.data\ntarget = fashion_data.target\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Setup data visualization\ndef plot_digit(image_data):\n    image = image_data.reshape(28, 28)\n    plt.imshow(image, cmap=\"binary\")\n    plt.axis(\"off\")\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n\n# Plot example data\nplt.figure(figsize=(9, 9))\nfor idx, image_data in enumerate(data[:10]):\n    plt.subplot(10, 10, idx + 1)\n    plot_digit(image_data)\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](code_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nThe figure above shows a sample of the fashion mnist dataset. The calculations and visuals below will reference this dataset.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.metrics import precision_score\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import cross_val_predict\n\n# create the target vector\nxTrain, xTest, yTrain, yTest = data[:1000], data[1000:], target[:1000], target[1000:]\nyTrainGood = (yTrain == '7')  \nyTestGood = (yTest == '7')\n\n# train the classifier\nclassifier = SGDClassifier()\nclassifier.fit(xTrain, yTrainGood)\nyTrainPrediction = cross_val_predict(classifier, xTrain, yTrainGood, cv=3)\n\n# get precision score\nprint(\"The presicion score after training the binary classifier: \", precision_score(yTrainGood, yTrainPrediction))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe presicion score after training the binary classifier:  0.859504132231405\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.metrics import recall_score\n\n# get recall score\nprint(\"The recall score of the classifier: \", recall_score(yTrainGood, yTrainPrediction))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe recall score of the classifier:  0.9043478260869565\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.metrics import f1_score\n\n# get F1 score\nprint(\"The F1 score of the classifier: \", f1_score(yTrainGood, yTrainPrediction))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe F1 score of the classifier:  0.8813559322033899\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.metrics import precision_recall_curve\n\nthreshold = 1000\nyScores = cross_val_predict(classifier, xTrain, yTrainGood, cv=3, method=\"decision_function\")\n\n# plot and format the PR curve\nprecisions, recalls, thresholds = precision_recall_curve(yTrainGood, yScores)\nidx = (thresholds > threshold).argmax() \n\nplt.figure(figsize=(6, 5)) \nplt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n\nplt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\nplt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\n\nplt.plot([recalls[idx]], [precisions[idx]], \"ko\",\n         label=\"Point at threshold 1000\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.axis([0, 1, 0, 1])\nplt.grid()\nplt.legend(loc=\"lower left\")\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<matplotlib.legend.Legend at 0x26682627fa0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](code_files/figure-html/cell-7-output-2.png){}\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.metrics import roc_curve\n\n# calculate roc\nprecisionIdx = (precisions >= 0.90).argmax()\nthresholdPrecision = thresholds[precisionIdx]\nfalsePositiveRate, truePositiveRate, thresholds = roc_curve(yTrainGood, yScores)\nidxThreshold = (thresholds <= thresholdPrecision).argmax()\ntruePositiveRate90, falsePositiveRate90 = truePositiveRate[idxThreshold], falsePositiveRate[idxThreshold]\n\n# plot the roc curve\nplt.figure(figsize=(6, 5))\nplt.plot(falsePositiveRate, truePositiveRate, linewidth=2, label=\"ROC curve\")\nplt.plot([falsePositiveRate90], [truePositiveRate90], \"ko\", label=\"Threshold for 90% precision\")\n\n# format the figure\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid()\nplt.axis([0, 1, 0, 1])\nplt.legend(loc=\"lower right\", fontsize=13)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](code_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "code_files"
    ],
    "filters": [],
    "includes": {}
  }
}