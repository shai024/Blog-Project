{
  "hash": "e095ba97541d2f7bcdadeced36a8a4f0",
  "result": {
    "markdown": "---\ntitle: \"Classification\"\nauthor: \"Britney Aiken\"\ndate: \"2023-12-04\"\nimage: \"classification.jpg\"\ncode-fold: true\ncode-tools: true\ncode-block-bg: true\ncode-block-border-left: \"#31BAE9\"\ncode-summary: \"Show code\"\norder: 2\n---\n\n![Data classifcation is the foundation of many machine learing models. It allows us to make informed decisions based on patterns in the data.](classification.jpg)\n\n\n### What is Data Classification? Why is it Important in Machine Learning?\nData classification is the process of organizing and labeling data into predefined categories or classes. The goal is to train a machine learning model to recognize patterns within the data and accurately assign new instances to the appropriate class. The importance of data classification in machine learning lies in its ability to enable intelligent decision-making. By categorizing data, models can generalize from past experiences to make predictions or classifications on new, unseen data. This capability forms the foundation for a wide array of applications that impact our daily lives.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport sys\nfrom packaging import version\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nimport matplotlib.pyplot as plt\n\n# Import the dataset\nfashion_data = fetch_openml('fashion-mnist', as_frame=False)\ndata = fashion_data.data \ntarget = fashion_data.target\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\britn\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n```\n:::\n:::\n\n\nReferences: [@Xiao]\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Setup data visualization\ndef plot_digit(image_data):\n    image = image_data.reshape(28, 28)\n    plt.imshow(image, cmap=\"binary\")\n    plt.axis(\"off\")\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n\n# Plot example data\nplt.figure(figsize=(9, 9))\nfor idx, image_data in enumerate(data[:10]):\n    plt.subplot(10, 10, idx + 1)\n    plot_digit(image_data)\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nThe figure above shows a sample of the fashion mnist dataset. The calculations and visuals below will reference this dataset. References: [@Geron]\n\n\n\n### Binary Classifiers\nBinary classification involves categorizing instances into one of two classes. These classes are usually boolean values(true or false; yes or no, 0 or 1). Evaluating the performance of a classifier is crucial to understand how well it generalizes to new data. Common performance measures include accuracy, precision, recall, F1 score, and the ROC-AUC curve. These metrics provide insights into the classifier's strengths and weaknesses,\n\n\n#### Confusion Matrix\nA confusion matrix provides a detailed breakdown of the model's predictions and the actual outcomes for each class. The four components of a confusion matrix are true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These components are used to calculate performance metrics for a binary classifier.\n\n**True Positives (TP):** The number of instances that are actually positive and are correctly predicted as positive\n\n**True Negatives (TN):** The number of instances that are actually negative and are correctly predicted as negative\n\n**False Positives (FP):** The number of instances that are actually negative but are incorrectly predicted as positive (Type 1 error)\n\n**False Negatives (FN):** The number of instances that are actually positive but are incorrectly predicted as negative (Type 2 error)\n\nUsing these components, we can calculate several performance metrics. \n\n**Accuracy:** The proportion of correctly classified instances out of the total instances. `(TP+TN)/(TP+TN+FP+FN)`\n\n**Precision** The ratio of true positives to the total predicted positives. `TP/(TP+FP)`\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.metrics import precision_score\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import cross_val_predict\n\n# create the target vector\nxTrain, xTest, yTrain, yTest = data[:1000], data[1000:], target[:1000], target[1000:]\nyTrainGood = (yTrain == '7')  \nyTestGood = (yTest == '7')\n\n# train the classifier\nclassifier = SGDClassifier()\nclassifier.fit(xTrain, yTrainGood)\nyTrainPrediction = cross_val_predict(classifier, xTrain, yTrainGood, cv=3)\n\n# get precision score\nprint(\"The presicion score after training the binary classifier: \", precision_score(yTrainGood, yTrainPrediction))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe presicion score after training the binary classifier:  0.8387096774193549\n```\n:::\n:::\n\n\n**Recall:** The ratio of true positives to the total actual positives. `TP/(TP+FN)` This metric is also known as Sensitivity or True Positive Rate\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.metrics import recall_score\n\n# get recall score\nprint(\"The recall score of the classifier: \", recall_score(yTrainGood, yTrainPrediction))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe recall score of the classifier:  0.9043478260869565\n```\n:::\n:::\n\n\n**Specificity:** The ratio of true negatives to the total actual negatives. `TN/(TN+FP)` This metric is also known as the True Negative Rate\n\n**F1 Score:** The harmonic mean of precision and recall. Increasing precision reduces recall, and vice versa. This is called the precision/recall trade-off. `2×(Precision×Recall)/(Precision+Recall)`\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.metrics import f1_score\n\n# get F1 score\nprint(\"The F1 score of the classifier: \", f1_score(yTrainGood, yTrainPrediction))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe F1 score of the classifier:  0.8702928870292886\n```\n:::\n:::\n\n\n#### The Precsion/Recall (PR) Curve\nThe precision-recall curve is created by plotting precision against recall at different threshold values. Each point on the curve corresponds to a specific decision threshold used by the classifier to make predictions.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.metrics import precision_recall_curve\n\nthreshold = 1000\nyScores = cross_val_predict(classifier, xTrain, yTrainGood, cv=3, method=\"decision_function\")\n\n# plot and format the PR curve\nprecisions, recalls, thresholds = precision_recall_curve(yTrainGood, yScores)\nidx = (thresholds > threshold).argmax() \n\nplt.figure(figsize=(6, 5)) \nplt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n\nplt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\nplt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\n\nplt.plot([recalls[idx]], [precisions[idx]], \"ko\",\n         label=\"Point at threshold 1000\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.axis([0, 1, 0, 1])\nplt.grid()\nplt.legend(loc=\"lower left\")\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<matplotlib.legend.Legend at 0x19929ff5fa0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){}\n:::\n:::\n\n\n#### The ROC Curve\nThe Receiver Operating Characteristic (ROC) curve is another tool used with binary classifiers. It is similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate (FPR).\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.metrics import roc_curve\n\n# calculate roc\nprecisionIdx = (precisions >= 0.90).argmax()\nthresholdPrecision = thresholds[precisionIdx]\nfalsePositiveRate, truePositiveRate, thresholds = roc_curve(yTrainGood, yScores)\nidxThreshold = (thresholds <= thresholdPrecision).argmax()\ntruePositiveRate90, falsePositiveRate90 = truePositiveRate[idxThreshold], falsePositiveRate[idxThreshold]\n\n# plot the roc curve\nplt.figure(figsize=(6, 5))\nplt.plot(falsePositiveRate, truePositiveRate, linewidth=2, label=\"ROC curve\")\nplt.plot([falsePositiveRate90], [truePositiveRate90], \"ko\", label=\"Threshold for 90% precision\")\n\n# format the figure\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid()\nplt.axis([0, 1, 0, 1])\nplt.legend(loc=\"lower right\", fontsize=13)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\nThe higher the recall (TPR), the more false positives (FPR) the classifier produces. One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. \n\n### Multiclass Classification\nIn multiclass classification, instances are assigned to one of multiple classes. The model learns decision boundaries to separate instances belonging to different classes. The decision-making process involves assigning each instance to the class with the highest probability. Two  strategies for adapting binary classifiers to multiclass problems are One-vs-All (OvA) and One-vs-One (OvO).\n\n**One-vs-All:** Train a separate binary classifier for each class, treating it as the positive class and the rest as the negative class. The class with the highest score is then predicted.\n\n**One-vs-One:** Train a binary classifier for every pair of classes. In the prediction phase, each classifier votes for a class, and the class with the most votes is the final prediction.\n\n\n### Other Types of Classification\nMultilabel classification allows an instance to be associated with multiple labels or classes at the same time. This scenario is common in real-world problems where data points have many attributes.\n\nMultioutput Classification is a generalization of multilabel classification where each label can be multiclass\n\n### References\nXiao, Han, et al. “Fashion-MNIST.” OpenML, openml.org/search?type=data&amp;status=active&amp;sort=qualities.NumberOfNumericFeatures&amp;id=40996. Accessed 15 Dec. 2023. \n\nGéron, Aurélien. Hands-on Machine Learning with Scikit-Learn, Keras, and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media, 2022. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}