{
  "hash": "5c1b37aecb90a350daeb08a5caeb0c12",
  "result": {
    "markdown": "---\ntitle: \"Anomaly/Outlier Detection\"\nauthor: \"Britney Aiken\"\ndate: \"2023-12-04\"\nimage: \"anomaly.png\"\ncode-fold: true\ncode-tools: true\ncode-block-bg: true\ncode-block-border-left: \"#31BAE9\"\n---\n\n![Anomaly detection is useful in a wide variety of applications, such as fraud detection, detecting defective products, and cybersecurity.](anomaly.png)\n\n### What is Anomaly Detection?\nThe goal is to learn what “normal” data looks like, and then use that to detect abnormal instances. These instances are called anomalies or outliers. \n\n### Why is it important?\nAnomaly detection is a critical tool for maintaining the integrity and security of a system. It enables early identification of unusual patterns that may causes issues.\n\n\n### Algorithms for Anamoly Detction\n\n**Gaussian Mixture**\nAnomaly detection assumes that normal instances occur more often than outliers. The algorithm starts by learning the patterns of normal behavior and analzes the datapoint that do not fit the pattern. When using a Gaussian mixture model for anomaly detection, any instance located in a low-density region can be considered an anomaly.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.mixture import GaussianMixture\nfrom matplotlib.colors import LogNorm\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights > weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z,\n                 norm=LogNorm(vmin=1.0, vmax=30.0),\n                 levels=np.logspace(0, 2, 12))\n    plt.contour(xx, yy, Z,\n                norm=LogNorm(vmin=1.0, vmax=30.0),\n                levels=np.logspace(0, 2, 12),\n                linewidths=1, colors='k')\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\ngm = GaussianMixture(n_components=3, n_init=10, random_state=42)\nX1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\nX1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\nX2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\nX2 = X2 + [6, -8]\nX = np.r_[X1, X2]\ngm.fit(X)\ndensities = gm.score_samples(X)\ndensity_threshold = np.percentile(densities, 2)\nanomalies = X[densities < density_threshold]\n\nplot_gaussian_mixture(gm, X)\nplt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='*')\nplt.ylim(top=5.1)\nplt.figure(figsize=(8, 4))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=581 height=430}\n:::\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 768x384 with 0 Axes>\n```\n:::\n:::\n\n\n**Fast-MCD (Minimum Covariance Determinant)**\nThis algorithm is useful for outlier detection, especially when trying to clean up a dataset. When the algorithm estimates the parameters of the Gaussian distribution, it ignores the instances that are most likely outlier, making it easier to identify them.\n\n**Isolation Forest**\nThis algorithm works well in high-dimensional datasets. It builds a random forest where each decision tree grows randomly. The datapoints gradually spread apart causing the anomalies to become isolated much fater than normal datapoints. \n\n**Local outlier factor (LOF)**\nThis algorithm compares the density of instances around a given instance to the density around its neighbors. An anomaly is often more isolated than its k-nearest neighbors.\n\n**One-class SVM**\nThis algorithm is works well for novelty detection. Novelty detection differs from anomaly detection by assuming the algorithm was trained on a “clean” dataset, with no outliers. One-class SVM works by finding a small region that encompasses all the instances. If a new instance does not fall within this region, it is an anomaly\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}