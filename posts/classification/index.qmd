---
title: "Classification"
author: "Britney Aiken"
date: "2023-12-04"
image: "classification.jpg"
code-fold: true
code-tools: true
code-block-bg: true
code-block-border-left: "#31BAE9"
code-summary: "Show code"
---

![Data classifcation is the foundation of many machine learing models. It allows us to make informed decisions based on patterns in the data.](classification.jpg)


### What is Data Classification? Why is it Important in Machine Learning?
Data classification is the process of organizing and labeling data into predefined categories or classes. The goal is to train a machine learning model to recognize patterns within the data and accurately assign new instances to the appropriate class. The importance of data classification in machine learning lies in its ability to enable intelligent decision-making. By categorizing data, models can generalize from past experiences to make predictions or classifications on new, unseen data. This capability forms the foundation for a wide array of applications that impact our daily lives.


```{python}
import sys
from packaging import version
import sklearn
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt

# Setup data visualization
def plot_digit(image_data):
    image = image_data.reshape(28, 28)
    plt.imshow(image, cmap="binary")
    plt.axis("off")

plt.rc('font', size=14)
plt.rc('axes', labelsize=14, titlesize=14)
plt.rc('legend', fontsize=14)
plt.rc('xtick', labelsize=10)
plt.rc('ytick', labelsize=10)

# Import the dataset
mnist = fetch_openml('mnist_784', as_frame=False)
X, y = mnist.data, mnist.target


# Plot example data
plt.figure(figsize=(9, 9))
for idx, image_data in enumerate(X[:100]):
    plt.subplot(10, 10, idx + 1)
    plot_digit(image_data)
plt.subplots_adjust(wspace=0, hspace=0)
plt.show()

```
The figure above shows a sample of the MNIST dataset. The calculations and visuals below will reference this dataset.

### Binary Classifiers
Binary classification involves categorizing instances into one of two classes. These classes are usually boolean values(true or false; yes or no, 0 or 1). Evaluating the performance of a classifier is crucial to understand how well it generalizes to new data. Common performance measures include accuracy, precision, recall, F1 score, and the ROC-AUC curve. These metrics provide insights into the classifier's strengths and weaknesses,


```{python}
some_digit = X[7]
plot_digit(some_digit)
plt.show()
```
We will train a binary classifier for the number `3`

#### Confusion Matrix
A confusion matrix provides a detailed breakdown of the model's predictions and the actual outcomes for each class. The four components of a confusion matrix are true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These components are used to calculate performance metrics for a binary classifier.

**True Positives (TP):** The number of instances that are actually positive and are correctly predicted as positive

**True Negatives (TN):** The number of instances that are actually negative and are correctly predicted as negative

**False Positives (FP):** The number of instances that are actually negative but are incorrectly predicted as positive (Type 1 error)

**False Negatives (FN):** The number of instances that are actually positive but are incorrectly predicted as negative (Type 2 error)

Using these components, we can calculate several performance metrics. 

**Accuracy:** The proportion of correctly classified instances out of the total instances. `(TP+TN)/(TP+TN+FP+FN)`

**Precision** The ratio of true positives to the total predicted positives. `TP/(TP+FP)`
```{python}
from sklearn.metrics import precision_score
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import cross_val_predict

# create the target vector
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]
y_train_3 = (y_train == '3')  
y_test_3 = (y_test == '3')

# train the classifier
sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(X_train, y_train_3)
y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_3, cv=3)

# get precision score
print("The presicion score after training the binary classifier: ", precision_score(y_train_3, y_train_pred))
```

**Recall:** The ratio of true positives to the total actual positives. `TP/(TP+FN)` This metric is also known as Sensitivity or True Positive Rate
```{python}
from sklearn.metrics import recall_score

# get recall score
print("The recall score of the classifier: ", recall_score(y_train_3, y_train_pred))
```

**Specificity:** The ratio of true negatives to the total actual negatives. `TN/(TN+FP)` This metric is also known as the True Negative Rate

**F1 Score:** The harmonic mean of precision and recall. Increasing precision reduces recall, and vice versa. This is called the precision/recall trade-off. `2×(Precision×Recall)/(Precision+Recall)`
```{python}
from sklearn.metrics import f1_score

# get F1 score
print("The F1 score of the classifier: ", f1_score(y_train_3, y_train_pred))
```


#### The Precsion/Recall (PR) Curve
The precision-recall curve is created by plotting precision against recall at different threshold values. Each point on the curve corresponds to a specific decision threshold used by the classifier to make predictions.
```{python}
from sklearn.metrics import precision_recall_curve

threshold = 3000
y_scores = cross_val_predict(sgd_clf, X_train, y_train_3, cv=3, method="decision_function")

# plot and format the PR curve
precisions, recalls, thresholds = precision_recall_curve(y_train_3, y_scores)
idx = (thresholds >= threshold).argmax() 

plt.figure(figsize=(6, 5)) 
plt.plot(recalls, precisions, linewidth=2, label="Precision/Recall curve")
plt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], "k:")
plt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], "k:")
plt.plot([recalls[idx]], [precisions[idx]], "ko",
         label="Point at threshold 3,000")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.axis([0, 1, 0, 1])
plt.grid()
plt.legend(loc="lower left")
```

#### The ROC Curve
The Receiver Operating Characteristic (ROC) curve is another tool used with binary classifiers. It is similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate (FPR).

```{python}
from sklearn.metrics import roc_curve

# calculate roc
idx_for_90_precision = (precisions >= 0.90).argmax()
threshold_for_90_precision = thresholds[idx_for_90_precision]
fpr, tpr, thresholds = roc_curve(y_train_3, y_scores)
idx_for_threshold_at_90 = (thresholds <= threshold_for_90_precision).argmax()
tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]

# plot the roc curve
plt.figure(figsize=(6, 5))  # extra code – not needed, just formatting
plt.plot(fpr, tpr, linewidth=2, label="ROC curve")
plt.plot([0, 1], [0, 1], 'k:', label="Random classifier's ROC curve")
plt.plot([fpr_90], [tpr_90], "ko", label="Threshold for 90% precision")

# format the figure
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.grid()
plt.axis([0, 1, 0, 1])
plt.legend(loc="lower right", fontsize=13)
plt.show()
```

The higher the recall (TPR), the more false positives (FPR) the classifier produces. One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. 

### Multiclass Classification
In multiclass classification, instances are assigned to one of multiple classes. The model learns decision boundaries to separate instances belonging to different classes. The decision-making process involves assigning each instance to the class with the highest probability. Two  strategies for adapting binary classifiers to multiclass problems are One-vs-All (OvA) and One-vs-One (OvO).

**One-vs-All:** Train a separate binary classifier for each class, treating it as the positive class and the rest as the negative class. The class with the highest score is then predicted.

**One-vs-One:** Train a binary classifier for every pair of classes. In the prediction phase, each classifier votes for a class, and the class with the most votes is the final prediction.


### Other Types of Classification
Multilabel classification allows an instance to be associated with multiple labels or classes at the same time. This scenario is common in real-world problems where data points have many attributes.

Multioutput Classification is a generalization of multilabel classification where each label can be multiclass
